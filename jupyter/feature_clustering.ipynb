{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46155001-a466-4d7f-971a-0723c96ce74e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cluster document vectors of extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b924da5-55a0-4355-989d-d71240540c13",
   "metadata": {},
   "source": [
    "## Load, extract vectors from DocuScope cluster counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9156ef-ac50-45cf-af90-22deab883d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>AcademicTerms</th>\n",
       "      <th>AcademicWritingMoves</th>\n",
       "      <th>...</th>\n",
       "      <th>Narrative</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>PublicTerms</th>\n",
       "      <th>Reasoning</th>\n",
       "      <th>Responsibility</th>\n",
       "      <th>Strategic</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Updates</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12834217_1</th>\n",
       "      <td>12834217</td>\n",
       "      <td>1</td>\n",
       "      <td>As of March 13th , 2014 , the booklet had been...</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_2</th>\n",
       "      <td>12834217</td>\n",
       "      <td>2</td>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_3</th>\n",
       "      <td>12834217</td>\n",
       "      <td>3</td>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_4</th>\n",
       "      <td>12834217</td>\n",
       "      <td>4</td>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_5</th>\n",
       "      <td>12834217</td>\n",
       "      <td>5</td>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677015_1</th>\n",
       "      <td>33677015</td>\n",
       "      <td>1</td>\n",
       "      <td>Apparently he came to the conclusion that his ...</td>\n",
       "      <td>572948</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677019_1</th>\n",
       "      <td>33677019</td>\n",
       "      <td>1</td>\n",
       "      <td>Wish we at least had a Marine Le Pen to vote f...</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677019_2</th>\n",
       "      <td>33677019</td>\n",
       "      <td>2</td>\n",
       "      <td>Its like the choices are white genocide candid...</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677053_1</th>\n",
       "      <td>33677053</td>\n",
       "      <td>1</td>\n",
       "      <td>Why White people used to say that sex was a si...</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677053_2</th>\n",
       "      <td>33677053</td>\n",
       "      <td>2</td>\n",
       "      <td>Now I get it !</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10913 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            comment_id  sentence_id  \\\n",
       "12834217_1    12834217            1   \n",
       "12834217_2    12834217            2   \n",
       "12834217_3    12834217            3   \n",
       "12834217_4    12834217            4   \n",
       "12834217_5    12834217            5   \n",
       "...                ...          ...   \n",
       "33677015_1    33677015            1   \n",
       "33677019_1    33677019            1   \n",
       "33677019_2    33677019            2   \n",
       "33677053_1    33677053            1   \n",
       "33677053_2    33677053            2   \n",
       "\n",
       "                                                         text  user_id  \\\n",
       "12834217_1  As of March 13th , 2014 , the booklet had been...   572066   \n",
       "12834217_2  In order to help increase the booklets downloa...   572066   \n",
       "12834217_3  ( Simply copy and paste the following text int...   572066   \n",
       "12834217_4  Click below for a FREE download of a colorfull...   572066   \n",
       "12834217_5  Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...   572066   \n",
       "...                                                       ...      ...   \n",
       "33677015_1  Apparently he came to the conclusion that his ...   572948   \n",
       "33677019_1  Wish we at least had a Marine Le Pen to vote f...   735154   \n",
       "33677019_2  Its like the choices are white genocide candid...   735154   \n",
       "33677053_1  Why White people used to say that sex was a si...   572266   \n",
       "33677053_2                                     Now I get it !   572266   \n",
       "\n",
       "            subforum_id  num_contexts   label  Tokens  AcademicTerms  \\\n",
       "12834217_1         1346             0  noHate      18              0   \n",
       "12834217_2         1346             0  noHate      36              1   \n",
       "12834217_3         1346             0  noHate      16              1   \n",
       "12834217_4         1346             0    hate      22              1   \n",
       "12834217_5         1346             0  noHate      22              0   \n",
       "...                 ...           ...     ...     ...            ...   \n",
       "33677015_1         1388             0  noHate      25              0   \n",
       "33677019_1         1388             0  noHate      15              0   \n",
       "33677019_2         1388             0  noHate      14              0   \n",
       "33677053_1         1388             0    hate      35              2   \n",
       "33677053_2         1388             0  noHate       5              0   \n",
       "\n",
       "            AcademicWritingMoves  ...  Narrative  Negative  Positive  \\\n",
       "12834217_1                     0  ...          2         0         0   \n",
       "12834217_2                     0  ...          1         0         0   \n",
       "12834217_3                     0  ...          0         0         0   \n",
       "12834217_4                     0  ...          0         1         1   \n",
       "12834217_5                     0  ...          0         0         0   \n",
       "...                          ...  ...        ...       ...       ...   \n",
       "33677015_1                     0  ...          2         2         0   \n",
       "33677019_1                     0  ...          1         0         0   \n",
       "33677019_2                     0  ...          0         1         0   \n",
       "33677053_1                     0  ...          4         1         0   \n",
       "33677053_2                     0  ...          1         0         0   \n",
       "\n",
       "            PublicTerms  Reasoning  Responsibility  Strategic  Uncertainty  \\\n",
       "12834217_1            1          0               0          0            0   \n",
       "12834217_2            2          1               0          1            0   \n",
       "12834217_3            1          0               0          0            0   \n",
       "12834217_4            0          0               0          0            0   \n",
       "12834217_5            0          0               0          0            0   \n",
       "...                 ...        ...             ...        ...          ...   \n",
       "33677015_1            0          2               0          1            0   \n",
       "33677019_1            0          0               0          1            0   \n",
       "33677019_2            0          0               0          0            0   \n",
       "33677053_1            0          0               0          0            1   \n",
       "33677053_2            0          0               0          0            0   \n",
       "\n",
       "            Updates  Group  \n",
       "12834217_1        0    NaN  \n",
       "12834217_2        0    NaN  \n",
       "12834217_3        0    NaN  \n",
       "12834217_4        0    NaN  \n",
       "12834217_5        0    NaN  \n",
       "...             ...    ...  \n",
       "33677015_1        0    NaN  \n",
       "33677019_1        0    NaN  \n",
       "33677019_2        0    NaN  \n",
       "33677053_1        0    NaN  \n",
       "33677053_2        0    NaN  \n",
       "\n",
       "[10913 rows x 45 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load category counts\n",
    "import pandas as pd\n",
    "\n",
    "csvpath = '/storage2/mamille3/data/hate_speech/degibert2019/docuscope_output/sentences-2022-02-18-131521/csv/CLUSTER_C_sentences.csv'\n",
    "category_counts = pd.read_csv(csvpath, index_col=0)\n",
    "category_counts.index = category_counts.index.str.slice(0,-4)\n",
    "category_counts\n",
    "len(category_counts.columns)\n",
    "\n",
    "old_cols = category_counts.columns\n",
    "\n",
    "# Drop categories that do not occur\n",
    "category_counts = category_counts.loc[:, (category_counts != 0).any(axis=0)] # Is just one category\n",
    "len(category_counts.columns)\n",
    "\n",
    "# Show categories that didn't occur\n",
    "new_cols = category_counts.columns\n",
    "print(set(old_cols) - set(new_cols))\n",
    "\n",
    "# Load sentence splits and annotations\n",
    "annotations_fpath = '/storage2/mamille3/data/hate_speech/degibert2019/combined_data.csv'\n",
    "annotations = pd.read_csv(annotations_fpath).sort_values(['comment_id', 'sentence_id']).set_index('file_id')\n",
    "annotations\n",
    "\n",
    "# Merge DocuScope output with labels, metadata\n",
    "merged = pd.merge(annotations, category_counts, left_index=True, right_index=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f25539-3d5e-42a0-b666-4332f08ccf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>AcademicTerms</th>\n",
       "      <th>mean_AcademicTerms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12834217_1</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_2</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_3</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_4</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12834217_5</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677015_1</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677019_1</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677019_2</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677053_1</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33677053_2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10913 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Tokens  AcademicTerms  mean_AcademicTerms\n",
       "12834217_1      18              0            0.000000\n",
       "12834217_2      36              1            0.027778\n",
       "12834217_3      16              1            0.062500\n",
       "12834217_4      22              1            0.045455\n",
       "12834217_5      22              0            0.000000\n",
       "...            ...            ...                 ...\n",
       "33677015_1      25              0            0.000000\n",
       "33677019_1      15              0            0.000000\n",
       "33677019_2      14              0            0.000000\n",
       "33677053_1      35              2            0.057143\n",
       "33677053_2       5              0            0.000000\n",
       "\n",
       "[10913 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize count vectors (just by token length, though could do the whole scaling thing to unit variance)\n",
    "# Or could take the log or something so it's not such tiny fractions\n",
    "for col in merged.columns[8:]:\n",
    "    merged[f'mean_{col}'] = merged[col]/merged['Tokens']\n",
    "merged.loc[:, ['Tokens', 'AcademicTerms', 'mean_AcademicTerms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f59197-ec29-4b9a-877c-91896bc02d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10913, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract normalized count vectors (or regular)\n",
    "vectors = merged[[col for col in merged.columns if 'mean_' in col]].values\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9605d670-21e5-41a4-b079-f39b2f9a6dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaNs\n",
    "import numpy as np\n",
    "\n",
    "np.isnan(np.min(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadcf6f3-88a8-4575-add1-7dd8c5d4f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10913, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = np.nan_to_num(vectors)\n",
    "print(processed.shape)\n",
    "np.isnan(np.min(processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7f4416-03a5-44fa-858d-a32db3954f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10913, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=5) 45.2% variance explained\n",
    "pca = PCA(n_components=.8, svd_solver='full')\n",
    "reduced = pca.fit_transform(processed)\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058fb51b-da0a-4727-9b29-1ca6a49279e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8264181637601749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.16080998, 0.11603921, 0.0897078 , 0.08728835, 0.07682701,\n",
       "       0.0744396 , 0.06902329, 0.05452897, 0.04026241, 0.02936255,\n",
       "       0.02812899])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(pca.explained_variance_ratio_))\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7804938-039f-4e73-abf1-0de65c5cc909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef2446b-aba6-4fa6-9615-343caa623c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top DocuScope features for each component\n",
    "def feats_for_factors(feature_names, pca, n_factors=20, n_feats=40):\n",
    "    top = np.flip(np.argsort(pca.components_)[:n_factors, -1*n_feats:], axis=1)\n",
    "    vec = np.vectorize(lambda x: feature_names[x])\n",
    "    return vec(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac810a1-e594-4e5b-8220-df600a8b1769",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_AcademicTerms',\n",
       " 'mean_AcademicWritingMoves',\n",
       " 'mean_Character',\n",
       " 'mean_Citation',\n",
       " 'mean_CitationAuthority',\n",
       " 'mean_CitationHedged',\n",
       " 'mean_ConfidenceHedged',\n",
       " 'mean_ConfidenceHigh',\n",
       " 'mean_ConfidenceLow',\n",
       " 'mean_Contingent',\n",
       " 'mean_Description',\n",
       " 'mean_Facilitate',\n",
       " 'mean_FirstPerson',\n",
       " 'mean_ForceStressed',\n",
       " 'mean_Future',\n",
       " 'mean_InformationChange',\n",
       " 'mean_InformationChangeNegative',\n",
       " 'mean_InformationChangePositive',\n",
       " 'mean_InformationExposition',\n",
       " 'mean_InformationPlace',\n",
       " 'mean_InformationReportVerbs',\n",
       " 'mean_InformationStates',\n",
       " 'mean_InformationTopics',\n",
       " 'mean_Inquiry',\n",
       " 'mean_Interactive',\n",
       " 'mean_MetadiscourseCohesive',\n",
       " 'mean_MetadiscourseInteractive',\n",
       " 'mean_Narrative',\n",
       " 'mean_Negative',\n",
       " 'mean_Positive',\n",
       " 'mean_PublicTerms',\n",
       " 'mean_Reasoning',\n",
       " 'mean_Responsibility',\n",
       " 'mean_Strategic',\n",
       " 'mean_Uncertainty',\n",
       " 'mean_Updates',\n",
       " 'mean_Group']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = [col for col in merged.columns if 'mean_' in col]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d4b83a-9e8b-4be8-9ec0-6960e863e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['mean_Character', 'mean_InformationStates', 'mean_Group',\n",
       "        'mean_CitationHedged', 'mean_ConfidenceLow'],\n",
       "       ['mean_Description', 'mean_Character',\n",
       "        'mean_InformationExposition', 'mean_FirstPerson',\n",
       "        'mean_InformationChange'],\n",
       "       ['mean_Positive', 'mean_ForceStressed', 'mean_Description',\n",
       "        'mean_Character', 'mean_MetadiscourseInteractive'],\n",
       "       ['mean_Interactive', 'mean_Description', 'mean_Positive',\n",
       "        'mean_ForceStressed', 'mean_Character'],\n",
       "       ['mean_Negative', 'mean_ForceStressed', 'mean_AcademicTerms',\n",
       "        'mean_MetadiscourseCohesive', 'mean_InformationStates'],\n",
       "       ['mean_InformationExposition', 'mean_ForceStressed',\n",
       "        'mean_AcademicTerms', 'mean_Strategic', 'mean_InformationStates'],\n",
       "       ['mean_ForceStressed', 'mean_Narrative', 'mean_Interactive',\n",
       "        'mean_Character', 'mean_FirstPerson'],\n",
       "       ['mean_AcademicTerms', 'mean_InformationTopics',\n",
       "        'mean_PublicTerms', 'mean_InformationPlace', 'mean_FirstPerson'],\n",
       "       ['mean_AcademicTerms', 'mean_Narrative', 'mean_Negative',\n",
       "        'mean_Positive', 'mean_Description'],\n",
       "       ['mean_InformationTopics', 'mean_PublicTerms', 'mean_Narrative',\n",
       "        'mean_InformationPlace', 'mean_ForceStressed'],\n",
       "       ['mean_InformationTopics', 'mean_FirstPerson', 'mean_Positive',\n",
       "        'mean_Negative', 'mean_MetadiscourseCohesive']], dtype='<U29')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topfeats = feats_for_factors(feature_names, pca, n_factors=11, n_feats=5)\n",
    "topfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4846cb63-0d0e-44d4-8ce7-d33917f565a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean_Character</td>\n",
       "      <td>mean_InformationStates</td>\n",
       "      <td>mean_Group</td>\n",
       "      <td>mean_CitationHedged</td>\n",
       "      <td>mean_ConfidenceLow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean_Description</td>\n",
       "      <td>mean_Character</td>\n",
       "      <td>mean_InformationExposition</td>\n",
       "      <td>mean_FirstPerson</td>\n",
       "      <td>mean_InformationChange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean_Positive</td>\n",
       "      <td>mean_ForceStressed</td>\n",
       "      <td>mean_Description</td>\n",
       "      <td>mean_Character</td>\n",
       "      <td>mean_MetadiscourseInteractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean_Interactive</td>\n",
       "      <td>mean_Description</td>\n",
       "      <td>mean_Positive</td>\n",
       "      <td>mean_ForceStressed</td>\n",
       "      <td>mean_Character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean_Negative</td>\n",
       "      <td>mean_ForceStressed</td>\n",
       "      <td>mean_AcademicTerms</td>\n",
       "      <td>mean_MetadiscourseCohesive</td>\n",
       "      <td>mean_InformationStates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mean_InformationExposition</td>\n",
       "      <td>mean_ForceStressed</td>\n",
       "      <td>mean_AcademicTerms</td>\n",
       "      <td>mean_Strategic</td>\n",
       "      <td>mean_InformationStates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mean_ForceStressed</td>\n",
       "      <td>mean_Narrative</td>\n",
       "      <td>mean_Interactive</td>\n",
       "      <td>mean_Character</td>\n",
       "      <td>mean_FirstPerson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mean_AcademicTerms</td>\n",
       "      <td>mean_InformationTopics</td>\n",
       "      <td>mean_PublicTerms</td>\n",
       "      <td>mean_InformationPlace</td>\n",
       "      <td>mean_FirstPerson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean_AcademicTerms</td>\n",
       "      <td>mean_Narrative</td>\n",
       "      <td>mean_Negative</td>\n",
       "      <td>mean_Positive</td>\n",
       "      <td>mean_Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mean_InformationTopics</td>\n",
       "      <td>mean_PublicTerms</td>\n",
       "      <td>mean_Narrative</td>\n",
       "      <td>mean_InformationPlace</td>\n",
       "      <td>mean_ForceStressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mean_InformationTopics</td>\n",
       "      <td>mean_FirstPerson</td>\n",
       "      <td>mean_Positive</td>\n",
       "      <td>mean_Negative</td>\n",
       "      <td>mean_MetadiscourseCohesive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0                       1  \\\n",
       "0               mean_Character  mean_InformationStates   \n",
       "1             mean_Description          mean_Character   \n",
       "2                mean_Positive      mean_ForceStressed   \n",
       "3             mean_Interactive        mean_Description   \n",
       "4                mean_Negative      mean_ForceStressed   \n",
       "5   mean_InformationExposition      mean_ForceStressed   \n",
       "6           mean_ForceStressed          mean_Narrative   \n",
       "7           mean_AcademicTerms  mean_InformationTopics   \n",
       "8           mean_AcademicTerms          mean_Narrative   \n",
       "9       mean_InformationTopics        mean_PublicTerms   \n",
       "10      mean_InformationTopics        mean_FirstPerson   \n",
       "\n",
       "                             2                           3  \\\n",
       "0                   mean_Group         mean_CitationHedged   \n",
       "1   mean_InformationExposition            mean_FirstPerson   \n",
       "2             mean_Description              mean_Character   \n",
       "3                mean_Positive          mean_ForceStressed   \n",
       "4           mean_AcademicTerms  mean_MetadiscourseCohesive   \n",
       "5           mean_AcademicTerms              mean_Strategic   \n",
       "6             mean_Interactive              mean_Character   \n",
       "7             mean_PublicTerms       mean_InformationPlace   \n",
       "8                mean_Negative               mean_Positive   \n",
       "9               mean_Narrative       mean_InformationPlace   \n",
       "10               mean_Positive               mean_Negative   \n",
       "\n",
       "                                4  \n",
       "0              mean_ConfidenceLow  \n",
       "1          mean_InformationChange  \n",
       "2   mean_MetadiscourseInteractive  \n",
       "3                  mean_Character  \n",
       "4          mean_InformationStates  \n",
       "5          mean_InformationStates  \n",
       "6                mean_FirstPerson  \n",
       "7                mean_FirstPerson  \n",
       "8                mean_Description  \n",
       "9              mean_ForceStressed  \n",
       "10     mean_MetadiscourseCohesive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(topfeats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
